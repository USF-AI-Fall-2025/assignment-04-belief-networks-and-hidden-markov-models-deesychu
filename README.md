# Belief-Networks-Hidden-Markov-Models
Fall 2025 CS 362/562

Reflection
1. The word "wired" was typed correctly but was changed to "wered" by the algorithm. This happened because the HMM has limited training data (only 17 examples of 'i'→'e' substitutions), so when comparing the probability sequences for "wired" versus "wered", the transition probabilities from the correct words in aspell.txt slightly favored the path through 'e' at that position. The emission probabilities alone weren't strong enough to keep the correct 'i', demonstrating how sparse training data can lead to incorrect corrections even when the original spelling was right.
2. The misspelled word "beleive" remained as "beleive" instead of being corrected to "believe". This occurs because the emission probability P(typed='e' | correct='i') = 0.11 is much weaker than P(typed='e' | correct='e') = 0.71, making the algorithm prefer to keep the 'e' rather than change it to 'i'. Even though the training data contains the "believe: beleive" mapping, the diagonal dominance (letters staying the same) combined with the small boost (+1) makes the Viterbi algorithm choose the path that keeps each typed letter unchanged when the emission probabilities strongly favor it.
3. Short words like "tge"→"tie" and "thw"→"the" were successfully corrected. These corrections work because with fewer characters, the transition probabilities play a larger role in the Viterbi decoding—for example, the strong transition probability P('h'|'t') and P('e'|'h') for the common word "the" can overcome weaker emission probabilities. With longer words like "beleive", the accumulated emission probabilities across more positions make it harder for transition probabilities to influence the final path, but in 3-letter words, a single strong transition can redirect the entire decode sequence toward the correct spelling.
4. Real typos from the internet would likely improve performance significantly because they would provide many more training examples and capture actual typing patterns (like common finger slips on QWERTY keyboards, phonetic misspellings, and autocorrect failures). The current aspell.txt dataset is small and potentially synthetic, resulting in sparse emission counts that make the diagonal (correct typing) too dominant. Real-world data would have higher counts for common confusions like 'i'↔'e', 'ei'↔'ie', and doubled letters, allowing the HMM to learn more accurate probabilities. Additionally, real typos would better represent the statistical distribution of errors people actually make, rather than artificial patterns, leading to more reliable corrections for common mistakes while still preserving correctly-spelled words.